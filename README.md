# Decoding Real-World Data using Information Theory

## Overview
This repository contains an individual university coursework project completed for
the module **Information Theory (5FTC2151)**. The project applies fundamental
information theory concepts to real-world text data using Python.

## Concepts Covered
- Shannon entropy computation
- Character frequency analysis
- Huffman coding (lossless compression)
- Compression efficiency vs theoretical entropy
- Hamming (7,4) code for error detection and correction
- Unicode-aware entropy analysis for non-Latin text (Sinhala)

## Datasets
- English text excerpt (Wikipedia – Entropy)
- Sinhala text excerpt (Wikipedia – Sri Lanka)
- UTF-8 encoding used for multilingual analysis

## Key Results
- English text entropy: **4.3159 bits/character**
- Sinhala text entropy: **4.8561 bits/character**
- Huffman encoding achieved near-optimal compression
- Single-bit errors corrected successfully using Hamming codes
- Demonstrated why two-bit errors lead to miscorrection

## Academic Context
This project was completed as an **individual coursework** during the 2nd year 1st Semester (2025).
